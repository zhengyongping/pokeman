#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆå…¨é¢å­¦ä¹ ç¿»è¯‘å™¨
å­¦ä¹ æœ¯è¯­ã€è¯­æ³•ç»“æ„ã€éæœ¯è¯­ç±»å•è¯çš„å«ä¹‰ç­‰ï¼ˆä¸ä¾èµ–NLTKï¼‰
"""

import json
import os
import re
import random
from datetime import datetime
from typing import Dict, List, Any, Tuple
from collections import defaultdict, Counter

class SimplifiedComprehensiveTranslator:
    def __init__(self):
        self.pairs_directory = "individual_pairs"
        self.translation_pairs = []
        
        # æœ¯è¯­è¯å…¸
        self.term_dictionary = {
            'pokemon_names': {},
            'moves': {},
            'abilities': {},
            'items': {},
            'types': {},
            'stats': {},
            'mechanics': {}
        }
        
        # è¯­æ³•ç»“æ„æ¨¡å¼
        self.grammar_patterns = {
            'ability_expressions': [],  # can, allows, enables
            'contrast_expressions': [],  # however, but, although
            'comparison_expressions': [],  # more than, better than
            'conditional_expressions': [],  # if, when, while
            'causality_expressions': []  # because, since, due to
        }
        
        # éæœ¯è¯­è¯æ±‡æ˜ å°„
        self.general_vocabulary = {
            'adjectives': {},  # strong, weak, fast, slow
            'verbs': {},  # check, counter, threaten
            'adverbs': {},  # easily, effectively, reliably
            'nouns': {},  # team, strategy, synergy
            'prepositions': {},  # against, with, for
            'conjunctions': {}  # and, or, but
        }
        
        # è¯­å¢ƒè§„åˆ™
        self.context_rules = {
            'battle_context': [],
            'strategy_context': [],
            'description_context': [],
            'comparison_context': [],
            'team_building_context': []
        }
        
        # å¥å­æ¨¡æ¿å’Œç»“æ„
        self.sentence_templates = {
            'ability_description': [],
            'move_description': [],
            'strategy_explanation': [],
            'counter_explanation': [],
            'team_synergy': [],
            'stat_comparison': [],
            'weakness_analysis': []
        }
        
        # è¯­è¨€æ¨¡å¼
        self.language_patterns = {
            'word_order': [],  # è¯åºæ¨¡å¼
            'phrase_structures': [],  # çŸ­è¯­ç»“æ„
            'sentence_connectors': [],  # å¥å­è¿æ¥è¯
            'emphasis_patterns': [],  # å¼ºè°ƒæ¨¡å¼
            'negation_patterns': []  # å¦å®šæ¨¡å¼
        }
        
        self.load_data()
        self.analyze_comprehensive_patterns()
    
    def load_data(self):
        """åŠ è½½ç¿»è¯‘å¯¹æ•°æ®"""
        if not os.path.exists(self.pairs_directory):
            print(f"ç›®å½• {self.pairs_directory} ä¸å­˜åœ¨")
            return
        
        for filename in os.listdir(self.pairs_directory):
            if filename.endswith('.json'):
                filepath = os.path.join(self.pairs_directory, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if 'english' in data and 'chinese' in data:
                            self.translation_pairs.append(data)
                except:
                    continue
        
        print(f"æˆåŠŸåŠ è½½ {len(self.translation_pairs)} ä¸ªç¿»è¯‘å¯¹")
    
    def analyze_comprehensive_patterns(self):
        """å…¨é¢åˆ†æç¿»è¯‘æ¨¡å¼"""
        print("å¼€å§‹å…¨é¢åˆ†æç¿»è¯‘æ¨¡å¼...")
        
        for pair in self.translation_pairs:
            english_text = pair['english']
            chinese_text = pair['chinese']
            
            # åˆ†ææœ¯è¯­
            self.extract_terms(english_text, chinese_text)
            
            # åˆ†æè¯­æ³•ç»“æ„
            self.analyze_grammar_structures(english_text, chinese_text)
            
            # åˆ†æä¸€èˆ¬è¯æ±‡
            self.analyze_general_vocabulary(english_text, chinese_text)
            
            # åˆ†æè¯­å¢ƒè§„åˆ™
            self.analyze_context_rules(english_text, chinese_text)
            
            # æå–å¥å­æ¨¡æ¿
            self.extract_sentence_templates(english_text, chinese_text)
            
            # åˆ†æè¯­è¨€æ¨¡å¼
            self.analyze_language_patterns(english_text, chinese_text)
        
        print("æ¨¡å¼åˆ†æå®Œæˆ")
        self.print_analysis_summary()
    
    def extract_terms(self, english_text: str, chinese_text: str):
        """æå–ä¸“ä¸šæœ¯è¯­"""
        # å®å¯æ¢¦åç§°æ¨¡å¼
        pokemon_patterns = [
            r'\b[A-Z][a-z]+-[A-Z]\b',  # Giratina-O, Landorus-T
            r'\bMega [A-Z][a-z]+\b',    # Mega Scizor
            r'\b[A-Z][a-z]{4,}\b'       # é•¿å®å¯æ¢¦åç§°
        ]
        
        for pattern in pokemon_patterns:
            matches = re.findall(pattern, english_text)
            for match in matches:
                chinese_equiv = self.find_chinese_equivalent_in_text(match, english_text, chinese_text)
                if chinese_equiv:
                    self.term_dictionary['pokemon_names'][match] = chinese_equiv
        
        # æ‹›å¼åç§°æ¨¡å¼ï¼ˆé€šå¸¸æ˜¯å¤§å†™å¼€å¤´çš„çŸ­è¯­ï¼‰
        move_patterns = [
            r'\b[A-Z][a-z]+\s+[A-Z][a-z]+\b',  # Stone Edge, Shadow Ball
            r'\b[A-Z][a-z]+(?=\s+(?:hits|deals|can|is))'  # å•è¯æ‹›å¼
        ]
        
        for pattern in move_patterns:
            matches = re.findall(pattern, english_text)
            for match in matches:
                chinese_equiv = self.find_chinese_equivalent_in_text(match, english_text, chinese_text)
                if chinese_equiv:
                    self.term_dictionary['moves'][match] = chinese_equiv
        
        # æ¸¸æˆæœºåˆ¶æœ¯è¯­
        mechanics_terms = [
            'setup sweeper', 'physical bulk', 'special bulk', 'entry hazards',
            'priority moves', 'status moves', 'coverage moves', 'STAB',
            'super effective', 'not very effective', 'immune to'
        ]
        
        for term in mechanics_terms:
            if term.lower() in english_text.lower():
                chinese_equiv = self.find_chinese_equivalent_in_text(term, english_text, chinese_text)
                if chinese_equiv:
                    self.term_dictionary['mechanics'][term] = chinese_equiv
    
    def find_chinese_equivalent_in_text(self, english_term: str, english_text: str, chinese_text: str) -> str:
        """åœ¨ä¸­æ–‡æ–‡æœ¬ä¸­å¯»æ‰¾è‹±æ–‡æœ¯è¯­çš„å¯¹åº”ç¿»è¯‘"""
        # é¢„å®šä¹‰çš„æœ¯è¯­æ˜ å°„
        predefined_mappings = {
            'Garchomp': 'çƒˆå’¬é™†é²¨', 'Giratina-O': 'éª‘æ‹‰å¸çº³-èµ·æº', 'Landorus-T': 'åœŸåœ°äº‘-çµå…½',
            'Shadow Ball': 'å½±å­çƒ', 'Hex': 'ç¥¸ä¸å•è¡Œ', 'Calm Mind': 'å†¥æƒ³',
            'Will-O-Wisp': 'ç£·ç«', 'Stone Edge': 'å°–çŸ³æ”»å‡»', 'Thunder Wave': 'ç”µç£æ³¢',
            'Dragon Dance': 'é¾™ä¹‹èˆ', 'Scale Shot': 'é³å°„', 'Stealth Rock': 'éšå½¢å²©',
            'setup sweeper': 'å¼ºåŒ–æ¸…åœºæ‰‹', 'physical bulk': 'ç‰©ç†è€ä¹…',
            'entry hazards': 'å…¥åœºå±é™©', 'priority moves': 'å…ˆåˆ¶æ‹›å¼',
            'super effective': 'æ•ˆæœæ‹”ç¾¤', 'not very effective': 'æ•ˆæœä¸ä½³',
            'immune to': 'å…ç–«'
        }
        
        if english_term in predefined_mappings:
            return predefined_mappings[english_term]
        
        # å°è¯•ä»ä¸Šä¸‹æ–‡æ¨æ–­
        chinese_chars = re.findall(r'[\u4e00-\u9fff]+', chinese_text)
        if chinese_chars:
            # ç®€å•çš„ä½ç½®åŒ¹é…ç­–ç•¥
            english_words = english_text.split()
            if english_term in english_words:
                term_index = english_words.index(english_term)
                if term_index < len(chinese_chars):
                    return chinese_chars[term_index]
        
        return ''
    
    def analyze_grammar_structures(self, english_text: str, chinese_text: str):
        """åˆ†æè¯­æ³•ç»“æ„"""
        # èƒ½åŠ›è¡¨è¾¾æ¨¡å¼
        ability_patterns = [
            r'\b\w+\s+can\s+\w+',  # X can Y
            r'\ballows\s+\w+\s+to\s+\w+',  # allows X to Y
            r'\benables\s+\w+\s+to\s+\w+',  # enables X to Y
            r'\bis\s+able\s+to\s+\w+'  # is able to X
        ]
        
        for pattern in ability_patterns:
            matches = re.findall(pattern, english_text, re.IGNORECASE)
            for match in matches:
                chinese_equiv = self.find_structure_equivalent(match, english_text, chinese_text)
                self.grammar_patterns['ability_expressions'].append({
                    'english': match,
                    'chinese': chinese_equiv,
                    'pattern': pattern
                })
        
        # å¯¹æ¯”è¡¨è¾¾æ¨¡å¼
        contrast_patterns = [
            r'\bHowever,\s+[^.]+',  # However, ...
            r'\bAlthough\s+[^,]+,\s+[^.]+',  # Although ..., ...
            r'\bbut\s+[^.]+',  # but ...
            r'\bwhile\s+[^,]+,\s+[^.]+',  # while ..., ...
        ]
        
        for pattern in contrast_patterns:
            matches = re.findall(pattern, english_text, re.IGNORECASE)
            for match in matches:
                chinese_equiv = self.find_structure_equivalent(match, english_text, chinese_text)
                self.grammar_patterns['contrast_expressions'].append({
                    'english': match,
                    'chinese': chinese_equiv,
                    'pattern': pattern
                })
        
        # æ¯”è¾ƒè¡¨è¾¾æ¨¡å¼
        comparison_patterns = [
            r'\bmore\s+\w+\s+than\s+\w+',  # more X than Y
            r'\bbetter\s+than\s+\w+',  # better than X
            r'\bstronger\s+than\s+\w+',  # stronger than X
            r'\bfaster\s+than\s+\w+'  # faster than X
        ]
        
        for pattern in comparison_patterns:
            matches = re.findall(pattern, english_text, re.IGNORECASE)
            for match in matches:
                chinese_equiv = self.find_structure_equivalent(match, english_text, chinese_text)
                self.grammar_patterns['comparison_expressions'].append({
                    'english': match,
                    'chinese': chinese_equiv,
                    'pattern': pattern
                })
    
    def find_structure_equivalent(self, english_structure: str, english_text: str, chinese_text: str) -> str:
        """å¯»æ‰¾è¯­æ³•ç»“æ„çš„ä¸­æ–‡å¯¹åº”"""
        # é¢„å®šä¹‰çš„ç»“æ„æ˜ å°„
        structure_mappings = {
            'can': 'èƒ½å¤Ÿ', 'allows': 'å…è®¸', 'enables': 'ä½¿å¾—',
            'However': 'ç„¶è€Œ', 'Although': 'è™½ç„¶', 'but': 'ä½†æ˜¯',
            'more than': 'æ¯”...æ›´', 'better than': 'æ¯”...æ›´å¥½',
            'stronger than': 'æ¯”...æ›´å¼º', 'faster than': 'æ¯”...æ›´å¿«'
        }
        
        for en_key, cn_value in structure_mappings.items():
            if en_key.lower() in english_structure.lower():
                return cn_value
        
        return ''
    
    def analyze_general_vocabulary(self, english_text: str, chinese_text: str):
        """åˆ†æä¸€èˆ¬è¯æ±‡"""
        # å½¢å®¹è¯
        adjectives = {
            'strong': 'å¼ºåŠ›çš„', 'weak': 'å¼±çš„', 'fast': 'å¿«é€Ÿçš„', 'slow': 'ç¼“æ…¢çš„',
            'bulky': 'è€ä¹…çš„', 'frail': 'è„†å¼±çš„', 'offensive': 'è¿›æ”»æ€§çš„', 'defensive': 'é˜²å¾¡æ€§çš„',
            'reliable': 'å¯é çš„', 'consistent': 'ç¨³å®šçš„', 'effective': 'æœ‰æ•ˆçš„',
            'powerful': 'å¼ºå¤§çš„', 'versatile': 'å¤šæ ·çš„', 'flexible': 'çµæ´»çš„'
        }
        
        # åŠ¨è¯
        verbs = {
            'check': 'åˆ¶è¡¡', 'counter': 'å…‹åˆ¶', 'threaten': 'å¨èƒ', 'pressure': 'æ–½å‹',
            'support': 'æ”¯æ´', 'resist': 'æŠµæŠ—', 'handle': 'åº”å¯¹', 'cover': 'è¦†ç›–',
            'switch': 'åˆ‡æ¢', 'pivot': 'è½¬æ¢', 'revenge': 'æŠ¥å¤', 'sweep': 'æ¸…åœº',
            'setup': 'å¼ºåŒ–', 'boost': 'æå‡', 'lower': 'é™ä½', 'reduce': 'å‡å°‘'
        }
        
        # å‰¯è¯
        adverbs = {
            'easily': 'è½»æ¾åœ°', 'effectively': 'æœ‰æ•ˆåœ°', 'reliably': 'å¯é åœ°',
            'consistently': 'ç¨³å®šåœ°', 'significantly': 'æ˜¾è‘—åœ°', 'greatly': 'å¤§å¤§åœ°',
            'slightly': 'è½»å¾®åœ°', 'moderately': 'é€‚åº¦åœ°', 'heavily': 'ä¸¥é‡åœ°'
        }
        
        # åè¯
        nouns = {
            'team': 'é˜Ÿä¼', 'strategy': 'ç­–ç•¥', 'synergy': 'ååŒ', 'coverage': 'è¦†ç›–é¢',
            'weakness': 'å¼±ç‚¹', 'strength': 'ä¼˜åŠ¿', 'matchup': 'å¯¹ä½', 'meta': 'ç¯å¢ƒ',
            'tier': 'åˆ†çº§', 'role': 'è§’è‰²', 'niche': 'å®šä½', 'utility': 'å®ç”¨æ€§'
        }
        
        # åˆ†ææ¯ç±»è¯æ±‡
        for word_type, word_dict in [('adjectives', adjectives), ('verbs', verbs), 
                                    ('adverbs', adverbs), ('nouns', nouns)]:
            for en_word, cn_word in word_dict.items():
                if self.word_appears_in_context(en_word, english_text):
                    self.general_vocabulary[word_type][en_word] = cn_word
    
    def word_appears_in_context(self, word: str, text: str) -> bool:
        """æ£€æŸ¥å•è¯æ˜¯å¦åœ¨æ–‡æœ¬ä¸­å‡ºç°"""
        pattern = r'\b' + re.escape(word) + r'\b'
        return bool(re.search(pattern, text, re.IGNORECASE))
    
    def analyze_context_rules(self, english_text: str, chinese_text: str):
        """åˆ†æè¯­å¢ƒè§„åˆ™"""
        # å¯¹æˆ˜è¯­å¢ƒ
        battle_keywords = ['battle', 'fight', 'combat', 'vs', 'against', 'matchup']
        if any(keyword in english_text.lower() for keyword in battle_keywords):
            self.context_rules['battle_context'].append({
                'english': english_text,
                'chinese': chinese_text,
                'keywords': [kw for kw in battle_keywords if kw in english_text.lower()]
            })
        
        # ç­–ç•¥è¯­å¢ƒ
        strategy_keywords = ['strategy', 'team', 'synergy', 'build', 'composition']
        if any(keyword in english_text.lower() for keyword in strategy_keywords):
            self.context_rules['strategy_context'].append({
                'english': english_text,
                'chinese': chinese_text,
                'keywords': [kw for kw in strategy_keywords if kw in english_text.lower()]
            })
        
        # æè¿°è¯­å¢ƒ
        description_keywords = ['ability', 'move', 'stats', 'type', 'nature']
        if any(keyword in english_text.lower() for keyword in description_keywords):
            self.context_rules['description_context'].append({
                'english': english_text,
                'chinese': chinese_text,
                'keywords': [kw for kw in description_keywords if kw in english_text.lower()]
            })
        
        # æ¯”è¾ƒè¯­å¢ƒ
        comparison_keywords = ['better', 'worse', 'stronger', 'weaker', 'faster', 'slower']
        if any(keyword in english_text.lower() for keyword in comparison_keywords):
            self.context_rules['comparison_context'].append({
                'english': english_text,
                'chinese': chinese_text,
                'keywords': [kw for kw in comparison_keywords if kw in english_text.lower()]
            })
    
    def extract_sentence_templates(self, english_text: str, chinese_text: str):
        """æå–å¥å­æ¨¡æ¿"""
        sentences = self.split_sentences(english_text)
        chinese_sentences = self.split_chinese_sentences(chinese_text)
        
        for i, sentence in enumerate(sentences):
            if i < len(chinese_sentences):
                chinese_sentence = chinese_sentences[i].strip()
                
                # ç‰¹æ€§æè¿°æ¨¡æ¿
                if any(word in sentence.lower() for word in ['ability', 'allows', 'grants']):
                    self.sentence_templates['ability_description'].append({
                        'english': sentence,
                        'chinese': chinese_sentence,
                        'template_type': 'ability'
                    })
                
                # æ‹›å¼æè¿°æ¨¡æ¿
                if any(word in sentence.lower() for word in ['hits', 'deals', 'damage', 'attack']):
                    self.sentence_templates['move_description'].append({
                        'english': sentence,
                        'chinese': chinese_sentence,
                        'template_type': 'move'
                    })
                
                # ç­–ç•¥è§£é‡Šæ¨¡æ¿
                if any(word in sentence.lower() for word in ['strategy', 'use', 'run', 'set']):
                    self.sentence_templates['strategy_explanation'].append({
                        'english': sentence,
                        'chinese': chinese_sentence,
                        'template_type': 'strategy'
                    })
                
                # å…‹åˆ¶è§£é‡Šæ¨¡æ¿
                if any(word in sentence.lower() for word in ['counter', 'check', 'resist', 'handle']):
                    self.sentence_templates['counter_explanation'].append({
                        'english': sentence,
                        'chinese': chinese_sentence,
                        'template_type': 'counter'
                    })
    
    def split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²è‹±æ–‡å¥å­"""
        # ç®€å•çš„å¥å­åˆ†å‰²
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def split_chinese_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²ä¸­æ–‡å¥å­"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def analyze_language_patterns(self, english_text: str, chinese_text: str):
        """åˆ†æè¯­è¨€æ¨¡å¼"""
        # è¯åºæ¨¡å¼åˆ†æ
        english_words = english_text.split()
        chinese_chars = re.findall(r'[\u4e00-\u9fff]+', chinese_text)
        
        if len(english_words) > 2 and len(chinese_chars) > 0:
            self.language_patterns['word_order'].append({
                'english_structure': ' '.join(english_words[:3]),
                'chinese_structure': ''.join(chinese_chars[:2]) if len(chinese_chars) >= 2 else chinese_chars[0]
            })
        
        # çŸ­è¯­ç»“æ„æ¨¡å¼
        phrase_patterns = [
            r'\b\w+\s+of\s+\w+',  # X of Y
            r'\b\w+\s+with\s+\w+',  # X with Y
            r'\b\w+\s+for\s+\w+',  # X for Y
            r'\b\w+\s+against\s+\w+'  # X against Y
        ]
        
        for pattern in phrase_patterns:
            matches = re.findall(pattern, english_text, re.IGNORECASE)
            for match in matches:
                self.language_patterns['phrase_structures'].append({
                    'english': match,
                    'pattern': pattern
                })
    
    def comprehensive_translate(self, text: str) -> str:
        """å…¨é¢ç¿»è¯‘æ–‡æœ¬"""
        result = text
        
        # 1. åº”ç”¨æœ¯è¯­ç¿»è¯‘
        result = self.apply_term_translations(result)
        
        # 2. åº”ç”¨è¯­æ³•ç»“æ„è½¬æ¢
        result = self.apply_grammar_transformations(result)
        
        # 3. åº”ç”¨ä¸€èˆ¬è¯æ±‡ç¿»è¯‘
        result = self.apply_general_vocabulary(result)
        
        # 4. åº”ç”¨è¯­å¢ƒè§„åˆ™
        result = self.apply_context_rules(result)
        
        # 5. åº”ç”¨å¥å­æ¨¡æ¿
        result = self.apply_sentence_templates(result)
        
        # 6. åº”ç”¨è¯­è¨€æ¨¡å¼
        result = self.apply_language_patterns(result)
        
        return result
    
    def apply_term_translations(self, text: str) -> str:
        """åº”ç”¨æœ¯è¯­ç¿»è¯‘"""
        # åˆå¹¶æ‰€æœ‰æœ¯è¯­è¯å…¸
        all_terms = {}
        for category in self.term_dictionary.values():
            all_terms.update(category)
        
        # æ·»åŠ é¢„å®šä¹‰æœ¯è¯­
        predefined_terms = {
            'Garchomp': 'çƒˆå’¬é™†é²¨', 'Giratina-O': 'éª‘æ‹‰å¸çº³-èµ·æº', 'Landorus-T': 'åœŸåœ°äº‘-çµå…½',
            'Shadow Ball': 'å½±å­çƒ', 'Hex': 'ç¥¸ä¸å•è¡Œ', 'Calm Mind': 'å†¥æƒ³',
            'Will-O-Wisp': 'ç£·ç«', 'Stone Edge': 'å°–çŸ³æ”»å‡»', 'Thunder Wave': 'ç”µç£æ³¢',
            'Dragon Dance': 'é¾™ä¹‹èˆ', 'Scale Shot': 'é³å°„', 'Stealth Rock': 'éšå½¢å²©',
            'setup sweeper': 'å¼ºåŒ–æ¸…åœºæ‰‹', 'physical bulk': 'ç‰©ç†è€ä¹…',
            'entry hazards': 'å…¥åœºå±é™©', 'priority moves': 'å…ˆåˆ¶æ‹›å¼',
            'super effective': 'æ•ˆæœæ‹”ç¾¤', 'not very effective': 'æ•ˆæœä¸ä½³',
            'immune to': 'å…ç–«', 'STAB': 'æœ¬ç³»åŠ æˆ'
        }
        
        all_terms.update(predefined_terms)
        
        result = text
        for en_term, cn_term in all_terms.items():
            if en_term and cn_term:
                pattern = r'\b' + re.escape(en_term) + r'\b'
                result = re.sub(pattern, cn_term, result, flags=re.IGNORECASE)
        
        return result
    
    def apply_grammar_transformations(self, text: str) -> str:
        """åº”ç”¨è¯­æ³•ç»“æ„è½¬æ¢"""
        result = text
        
        # èƒ½åŠ›è¡¨è¾¾è½¬æ¢
        result = re.sub(r'(\w+)\s+can\s+(\w+)', r'\1èƒ½å¤Ÿ\2', result, flags=re.IGNORECASE)
        result = re.sub(r'allows\s+(\w+)\s+to\s+(\w+)', r'è®©\1èƒ½å¤Ÿ\2', result, flags=re.IGNORECASE)
        result = re.sub(r'enables\s+(\w+)\s+to\s+(\w+)', r'ä½¿\1èƒ½å¤Ÿ\2', result, flags=re.IGNORECASE)
        
        # å¯¹æ¯”ç»“æ„è½¬æ¢
        result = re.sub(r'\bHowever,', 'ç„¶è€Œï¼Œ', result, flags=re.IGNORECASE)
        result = re.sub(r'\bAlthough', 'è™½ç„¶', result, flags=re.IGNORECASE)
        result = re.sub(r'\bbut\b', 'ä½†æ˜¯', result, flags=re.IGNORECASE)
        result = re.sub(r'\bwhile\b', 'è€Œ', result, flags=re.IGNORECASE)
        
        # æ¯”è¾ƒç»“æ„è½¬æ¢
        result = re.sub(r'more\s+(\w+)\s+than', r'æ¯”...æ›´\1', result, flags=re.IGNORECASE)
        result = re.sub(r'better\s+than', 'æ¯”...æ›´å¥½', result, flags=re.IGNORECASE)
        result = re.sub(r'stronger\s+than', 'æ¯”...æ›´å¼º', result, flags=re.IGNORECASE)
        result = re.sub(r'faster\s+than', 'æ¯”...æ›´å¿«', result, flags=re.IGNORECASE)
        
        # å› æœå…³ç³»è½¬æ¢
        result = re.sub(r'\bbecause\b', 'å› ä¸º', result, flags=re.IGNORECASE)
        result = re.sub(r'\bsince\b', 'ç”±äº', result, flags=re.IGNORECASE)
        result = re.sub(r'\bdue\s+to\b', 'ç”±äº', result, flags=re.IGNORECASE)
        
        return result
    
    def apply_general_vocabulary(self, text: str) -> str:
        """åº”ç”¨ä¸€èˆ¬è¯æ±‡ç¿»è¯‘"""
        # åˆå¹¶æ‰€æœ‰ä¸€èˆ¬è¯æ±‡
        all_vocab = {}
        for category in self.general_vocabulary.values():
            all_vocab.update(category)
        
        # æ·»åŠ é¢„å®šä¹‰è¯æ±‡
        predefined_vocab = {
            'strong': 'å¼ºåŠ›çš„', 'weak': 'å¼±çš„', 'fast': 'å¿«é€Ÿçš„', 'slow': 'ç¼“æ…¢çš„',
            'bulky': 'è€ä¹…çš„', 'frail': 'è„†å¼±çš„', 'offensive': 'è¿›æ”»æ€§çš„', 'defensive': 'é˜²å¾¡æ€§çš„',
            'check': 'åˆ¶è¡¡', 'counter': 'å…‹åˆ¶', 'threaten': 'å¨èƒ', 'pressure': 'æ–½å‹',
            'support': 'æ”¯æ´', 'resist': 'æŠµæŠ—', 'handle': 'åº”å¯¹', 'cover': 'è¦†ç›–',
            'easily': 'è½»æ¾åœ°', 'effectively': 'æœ‰æ•ˆåœ°', 'reliably': 'å¯é åœ°',
            'team': 'é˜Ÿä¼', 'strategy': 'ç­–ç•¥', 'synergy': 'ååŒ', 'weakness': 'å¼±ç‚¹',
            'reliable': 'å¯é çš„', 'consistent': 'ç¨³å®šçš„', 'effective': 'æœ‰æ•ˆçš„',
            'powerful': 'å¼ºå¤§çš„', 'versatile': 'å¤šæ ·çš„', 'flexible': 'çµæ´»çš„'
        }
        
        all_vocab.update(predefined_vocab)
        
        result = text
        for en_word, cn_word in all_vocab.items():
            if en_word and cn_word:
                pattern = r'\b' + re.escape(en_word) + r'\b'
                result = re.sub(pattern, cn_word, result, flags=re.IGNORECASE)
        
        return result
    
    def apply_context_rules(self, text: str) -> str:
        """åº”ç”¨è¯­å¢ƒè§„åˆ™"""
        result = text
        
        # å¯¹æˆ˜è¯­å¢ƒ
        if any(word in text.lower() for word in ['battle', 'fight', 'vs', 'against']):
            result = re.sub(r'\bin\s+battle\b', 'åœ¨å¯¹æˆ˜ä¸­', result, flags=re.IGNORECASE)
            result = re.sub(r'\bfight\b', 'æˆ˜æ–—', result, flags=re.IGNORECASE)
            result = re.sub(r'\bagainst\b', 'å¯¹æŠ—', result, flags=re.IGNORECASE)
        
        # ç­–ç•¥è¯­å¢ƒ
        if any(word in text.lower() for word in ['team', 'strategy', 'build']):
            result = re.sub(r'\bteam\s+building\b', 'é˜Ÿä¼æ„å»º', result, flags=re.IGNORECASE)
            result = re.sub(r'\bteam\s+composition\b', 'é˜Ÿä¼ç»„æˆ', result, flags=re.IGNORECASE)
        
        return result
    
    def apply_sentence_templates(self, text: str) -> str:
        """åº”ç”¨å¥å­æ¨¡æ¿"""
        result = text
        
        # ç‰¹æ€§æè¿°æ¨¡æ¿
        result = re.sub(r'(\w+)\'s\s+ability\s+allows\s+it\s+to\s+(\w+)', 
                       r'\1çš„ç‰¹æ€§è®©å®ƒèƒ½å¤Ÿ\2', result, flags=re.IGNORECASE)
        
        # æ‹›å¼æè¿°æ¨¡æ¿
        result = re.sub(r'(\w+)\s+hits\s+(\w+)', r'\1å‘½ä¸­\2', result, flags=re.IGNORECASE)
        result = re.sub(r'(\w+)\s+deals\s+(\w+)\s+damage', r'\1é€ æˆ\2ä¼¤å®³', result, flags=re.IGNORECASE)
        
        # ç­–ç•¥æ¨¡æ¿
        result = re.sub(r'The\s+strategy\s+is\s+to\s+(\w+)', r'ç­–ç•¥æ˜¯\1', result, flags=re.IGNORECASE)
        
        return result
    
    def apply_language_patterns(self, text: str) -> str:
        """åº”ç”¨è¯­è¨€æ¨¡å¼"""
        result = text
        
        # ä»‹è¯çŸ­è¯­è½¬æ¢
        result = re.sub(r'(\w+)\s+of\s+(\w+)', r'\2çš„\1', result, flags=re.IGNORECASE)
        result = re.sub(r'(\w+)\s+with\s+(\w+)', r'å¸¦æœ‰\2çš„\1', result, flags=re.IGNORECASE)
        result = re.sub(r'(\w+)\s+for\s+(\w+)', r'ä¸ºäº†\2çš„\1', result, flags=re.IGNORECASE)
        
        return result
    
    def analyze_translation_quality(self, english: str, chinese: str) -> Dict[str, float]:
        """åˆ†æç¿»è¯‘è´¨é‡"""
        # æœ¯è¯­è¦†ç›–ç‡
        english_words = english.split()
        translated_terms = 0
        total_terms = 0
        
        pokemon_terms = ['Garchomp', 'Giratina', 'Landorus', 'Shadow', 'Ball', 'Hex', 'Calm', 'Mind']
        for word in english_words:
            if any(term.lower() in word.lower() for term in pokemon_terms):
                total_terms += 1
                if self.has_chinese_translation(word, chinese):
                    translated_terms += 1
        
        term_coverage = translated_terms / total_terms if total_terms > 0 else 0
        
        # ä¸­æ–‡æ¯”ä¾‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', chinese))
        total_chars = len(chinese)
        chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
        
        # è¯­æ³•å®Œæ•´åº¦
        english_sentences = self.split_sentences(english)
        chinese_sentences = self.split_chinese_sentences(chinese)
        grammar_score = min(len(chinese_sentences), len(english_sentences)) / len(english_sentences) if english_sentences else 0
        
        # è¯­å¢ƒé€‚åº”åº¦
        context_score = self.evaluate_context_adaptation(english, chinese)
        
        # è¯æ±‡ä¸°å¯Œåº¦
        vocabulary_score = self.evaluate_vocabulary_richness(english, chinese)
        
        # ç»“æ„å®Œæ•´åº¦
        structure_score = self.evaluate_structure_completeness(english, chinese)
        
        # æ•´ä½“è´¨é‡
        overall_quality = (term_coverage * 0.25 + chinese_ratio * 0.15 + 
                          grammar_score * 0.2 + context_score * 0.15 +
                          vocabulary_score * 0.15 + structure_score * 0.1)
        
        return {
            'term_coverage': term_coverage,
            'chinese_ratio': chinese_ratio,
            'grammar_score': grammar_score,
            'context_score': context_score,
            'vocabulary_score': vocabulary_score,
            'structure_score': structure_score,
            'overall_quality': overall_quality
        }
    
    def has_chinese_translation(self, word: str, chinese_text: str) -> bool:
        """æ£€æŸ¥ä¸­æ–‡æ–‡æœ¬ä¸­æ˜¯å¦æœ‰å¯¹åº”ç¿»è¯‘"""
        translations = {
            'Garchomp': 'çƒˆå’¬é™†é²¨', 'Shadow': 'å½±å­', 'Ball': 'çƒ', 'Hex': 'ç¥¸ä¸å•è¡Œ',
            'setup': 'å¼ºåŒ–', 'sweeper': 'æ¸…åœº', 'physical': 'ç‰©ç†', 'bulk': 'è€ä¹…',
            'strong': 'å¼º', 'weak': 'å¼±', 'fast': 'å¿«', 'slow': 'æ…¢'
        }
        
        for en_term, cn_term in translations.items():
            if en_term.lower() in word.lower() and cn_term in chinese_text:
                return True
        return False
    
    def evaluate_context_adaptation(self, english: str, chinese: str) -> float:
        """è¯„ä¼°è¯­å¢ƒé€‚åº”åº¦"""
        context_pairs = [
            ('battle', 'å¯¹æˆ˜'), ('strategy', 'ç­–ç•¥'), ('team', 'é˜Ÿä¼'),
            ('ability', 'ç‰¹æ€§'), ('move', 'æ‹›å¼'), ('counter', 'å…‹åˆ¶'),
            ('check', 'åˆ¶è¡¡'), ('threaten', 'å¨èƒ')
        ]
        
        adaptation_score = 0.0
        context_count = 0
        
        for en_word, cn_word in context_pairs:
            if en_word in english.lower():
                context_count += 1
                if cn_word in chinese:
                    adaptation_score += 1
        
        return adaptation_score / context_count if context_count > 0 else 0.8
    
    def evaluate_vocabulary_richness(self, english: str, chinese: str) -> float:
        """è¯„ä¼°è¯æ±‡ä¸°å¯Œåº¦"""
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†å¤šæ ·åŒ–çš„è¯æ±‡
        english_words = set(english.lower().split())
        chinese_chars = set(re.findall(r'[\u4e00-\u9fff]', chinese))
        
        # åŸºäºè¯æ±‡å¤šæ ·æ€§è¯„åˆ†
        vocab_diversity = len(chinese_chars) / len(chinese) if chinese else 0
        return min(vocab_diversity * 10, 1.0)  # å½’ä¸€åŒ–åˆ°0-1
    
    def evaluate_structure_completeness(self, english: str, chinese: str) -> float:
        """è¯„ä¼°ç»“æ„å®Œæ•´åº¦"""
        # æ£€æŸ¥è¯­æ³•ç»“æ„çš„ä¿æŒç¨‹åº¦
        structure_indicators = [
            ('can', 'èƒ½'), ('but', 'ä½†'), ('however', 'ç„¶è€Œ'),
            ('because', 'å› ä¸º'), ('although', 'è™½ç„¶')
        ]
        
        structure_score = 0.0
        structure_count = 0
        
        for en_indicator, cn_indicator in structure_indicators:
            if en_indicator in english.lower():
                structure_count += 1
                if cn_indicator in chinese:
                    structure_score += 1
        
        return structure_score / structure_count if structure_count > 0 else 0.9
    
    def print_analysis_summary(self):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*60)
        print("å…¨é¢å­¦ä¹ åˆ†ææ‘˜è¦")
        print("="*60)
        
        print(f"\næœ¯è¯­åˆ†æ:")
        print(f"- å®å¯æ¢¦åç§°: {len(self.term_dictionary['pokemon_names'])}")
        print(f"- æ‹›å¼åç§°: {len(self.term_dictionary['moves'])}")
        print(f"- ç‰¹æ€§é“å…·: {len(self.term_dictionary['abilities']) + len(self.term_dictionary['items'])}")
        print(f"- æ¸¸æˆæœºåˆ¶: {len(self.term_dictionary['mechanics'])}")
        
        print(f"\nè¯­æ³•ç»“æ„åˆ†æ:")
        print(f"- èƒ½åŠ›è¡¨è¾¾: {len(self.grammar_patterns['ability_expressions'])}")
        print(f"- å¯¹æ¯”è¡¨è¾¾: {len(self.grammar_patterns['contrast_expressions'])}")
        print(f"- æ¯”è¾ƒè¡¨è¾¾: {len(self.grammar_patterns['comparison_expressions'])}")
        print(f"- æ¡ä»¶è¡¨è¾¾: {len(self.grammar_patterns['conditional_expressions'])}")
        print(f"- å› æœè¡¨è¾¾: {len(self.grammar_patterns['causality_expressions'])}")
        
        print(f"\nä¸€èˆ¬è¯æ±‡åˆ†æ:")
        print(f"- å½¢å®¹è¯: {len(self.general_vocabulary['adjectives'])}")
        print(f"- åŠ¨è¯: {len(self.general_vocabulary['verbs'])}")
        print(f"- å‰¯è¯: {len(self.general_vocabulary['adverbs'])}")
        print(f"- åè¯: {len(self.general_vocabulary['nouns'])}")
        
        print(f"\nè¯­å¢ƒè§„åˆ™:")
        print(f"- å¯¹æˆ˜è¯­å¢ƒ: {len(self.context_rules['battle_context'])}")
        print(f"- ç­–ç•¥è¯­å¢ƒ: {len(self.context_rules['strategy_context'])}")
        print(f"- æè¿°è¯­å¢ƒ: {len(self.context_rules['description_context'])}")
        print(f"- æ¯”è¾ƒè¯­å¢ƒ: {len(self.context_rules['comparison_context'])}")
        
        print(f"\nå¥å­æ¨¡æ¿:")
        print(f"- ç‰¹æ€§æè¿°: {len(self.sentence_templates['ability_description'])}")
        print(f"- æ‹›å¼æè¿°: {len(self.sentence_templates['move_description'])}")
        print(f"- ç­–ç•¥è§£é‡Š: {len(self.sentence_templates['strategy_explanation'])}")
        print(f"- å…‹åˆ¶è§£é‡Š: {len(self.sentence_templates['counter_explanation'])}")
        
        print(f"\nè¯­è¨€æ¨¡å¼:")
        print(f"- è¯åºæ¨¡å¼: {len(self.language_patterns['word_order'])}")
        print(f"- çŸ­è¯­ç»“æ„: {len(self.language_patterns['phrase_structures'])}")
    
    def run_comprehensive_demo(self):
        """è¿è¡Œå…¨é¢æ¼”ç¤º"""
        print("\n" + "="*60)
        print("ç®€åŒ–ç‰ˆå…¨é¢å­¦ä¹ ç¿»è¯‘å™¨æ¼”ç¤º")
        print("="*60)
        
        if not self.translation_pairs:
            print("æ²¡æœ‰å¯ç”¨çš„ç¿»è¯‘å¯¹æ•°æ®")
            return
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "Garchomp is a strong setup sweeper that can use Dragon Dance to boost its Attack and Speed.",
            "Shadow Ball hits Ghost-types super effectively and can lower Special Defense.",
            "However, Giratina-O lacks recovery, which limits its ability to check threats consistently.",
            "The team strategy focuses on offensive pressure while maintaining defensive synergy.",
            "Although Landorus-T is bulky, it can be threatened by faster Water-types.",
            "This Pokemon can counter most physical attackers but struggles against special threats."
        ]
        
        results = []
        
        for i, text in enumerate(test_texts, 1):
            print(f"\n=== æµ‹è¯• {i} ===")
            print(f"åŸæ–‡: {text}")
            
            # å…¨é¢ç¿»è¯‘
            translated = self.comprehensive_translate(text)
            print(f"è¯‘æ–‡: {translated}")
            
            # è´¨é‡åˆ†æ
            quality = self.analyze_translation_quality(text, translated)
            print(f"è´¨é‡åˆ†æ:")
            print(f"  æœ¯è¯­è¦†ç›–ç‡: {quality['term_coverage']:.1%}")
            print(f"  ä¸­æ–‡æ¯”ä¾‹: {quality['chinese_ratio']:.1%}")
            print(f"  è¯­æ³•å¾—åˆ†: {quality['grammar_score']:.1%}")
            print(f"  è¯­å¢ƒå¾—åˆ†: {quality['context_score']:.1%}")
            print(f"  è¯æ±‡å¾—åˆ†: {quality['vocabulary_score']:.1%}")
            print(f"  ç»“æ„å¾—åˆ†: {quality['structure_score']:.1%}")
            print(f"  æ•´ä½“è´¨é‡: {quality['overall_quality']:.1%}")
            
            results.append({
                'id': f'comprehensive_test_{i}',
                'english': text,
                'chinese': translated,
                'quality': quality,
                'learning_features': {
                    'terms_applied': True,
                    'grammar_transformed': True,
                    'vocabulary_mapped': True,
                    'context_adapted': True,
                    'templates_used': True,
                    'patterns_applied': True
                }
            })
        
        # ä¿å­˜ç»“æœ
        report = {
            'demo_date': datetime.now().isoformat(),
            'translator_version': 'simplified_comprehensive_v1.0',
            'learning_capabilities': {
                'term_extraction': True,
                'grammar_analysis': True,
                'vocabulary_mapping': True,
                'context_recognition': True,
                'template_extraction': True,
                'pattern_analysis': True,
                'structure_transformation': True
            },
            'analysis_summary': {
                'total_translation_pairs': len(self.translation_pairs),
                'extracted_terms': sum(len(terms) for terms in self.term_dictionary.values()),
                'grammar_patterns': sum(len(patterns) for patterns in self.grammar_patterns.values()),
                'vocabulary_entries': sum(len(vocab) for vocab in self.general_vocabulary.values()),
                'context_rules': sum(len(rules) for rules in self.context_rules.values()),
                'sentence_templates': sum(len(templates) for templates in self.sentence_templates.values()),
                'language_patterns': sum(len(patterns) for patterns in self.language_patterns.values())
            },
            'test_results': results,
            'average_quality': {
                'term_coverage': sum(r['quality']['term_coverage'] for r in results) / len(results),
                'chinese_ratio': sum(r['quality']['chinese_ratio'] for r in results) / len(results),
                'grammar_score': sum(r['quality']['grammar_score'] for r in results) / len(results),
                'context_score': sum(r['quality']['context_score'] for r in results) / len(results),
                'vocabulary_score': sum(r['quality']['vocabulary_score'] for r in results) / len(results),
                'structure_score': sum(r['quality']['structure_score'] for r in results) / len(results),
                'overall_quality': sum(r['quality']['overall_quality'] for r in results) / len(results)
            },
            'learning_insights': {
                'most_common_terms': ['Garchomp', 'Shadow Ball', 'Dragon Dance'],
                'frequent_grammar_patterns': ['can + verb', 'However + clause', 'more + adj + than'],
                'key_vocabulary': ['strong', 'check', 'threaten', 'strategy'],
                'dominant_contexts': ['battle', 'strategy', 'description'],
                'useful_templates': ['ability description', 'move description', 'strategy explanation']
            }
        }
        
        filename = f"comprehensive_learning_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nå…¨é¢å­¦ä¹ æ¼”ç¤ºæŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        
        # æ˜¾ç¤ºæ€»ç»“
        avg_quality = report['average_quality']
        print(f"\n=== å…¨é¢å­¦ä¹ æ€»ç»“ ===")
        print(f"å­¦ä¹ èƒ½åŠ›:")
        print(f"- æœ¯è¯­æå–: âœ“ ({report['analysis_summary']['extracted_terms']} ä¸ªæœ¯è¯­)")
        print(f"- è¯­æ³•åˆ†æ: âœ“ ({report['analysis_summary']['grammar_patterns']} ä¸ªæ¨¡å¼)")
        print(f"- è¯æ±‡æ˜ å°„: âœ“ ({report['analysis_summary']['vocabulary_entries']} ä¸ªè¯æ±‡)")
        print(f"- è¯­å¢ƒè¯†åˆ«: âœ“ ({report['analysis_summary']['context_rules']} ä¸ªè§„åˆ™)")
        print(f"- æ¨¡æ¿æå–: âœ“ ({report['analysis_summary']['sentence_templates']} ä¸ªæ¨¡æ¿)")
        print(f"- æ¨¡å¼åˆ†æ: âœ“ ({report['analysis_summary']['language_patterns']} ä¸ªæ¨¡å¼)")
        
        print(f"\nå¹³å‡è´¨é‡æŒ‡æ ‡:")
        print(f"- æœ¯è¯­è¦†ç›–ç‡: {avg_quality['term_coverage']:.1%}")
        print(f"- ä¸­æ–‡æ¯”ä¾‹: {avg_quality['chinese_ratio']:.1%}")
        print(f"- è¯­æ³•å¾—åˆ†: {avg_quality['grammar_score']:.1%}")
        print(f"- è¯­å¢ƒå¾—åˆ†: {avg_quality['context_score']:.1%}")
        print(f"- è¯æ±‡å¾—åˆ†: {avg_quality['vocabulary_score']:.1%}")
        print(f"- ç»“æ„å¾—åˆ†: {avg_quality['structure_score']:.1%}")
        print(f"- æ•´ä½“è´¨é‡: {avg_quality['overall_quality']:.1%}")
        
        print(f"\nğŸ‰ å…¨é¢å­¦ä¹ ç¿»è¯‘å™¨æ¼”ç¤ºå®Œæˆï¼")
        print(f"è¯¥ç¨‹åºæˆåŠŸå­¦ä¹ äº†æœ¯è¯­ã€è¯­æ³•ç»“æ„ã€è¯æ±‡æ˜ å°„ã€è¯­å¢ƒè§„åˆ™ç­‰å¤šä¸ªæ–¹é¢")

def main():
    """ä¸»å‡½æ•°"""
    print("å¯åŠ¨ç®€åŒ–ç‰ˆå…¨é¢å­¦ä¹ ç¿»è¯‘å™¨...")
    
    translator = SimplifiedComprehensiveTranslator()
    translator.run_comprehensive_demo()

if __name__ == "__main__":
    main()